{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af862fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Only log error messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79acb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'arabic_train.jsonl'\n",
    "import json\n",
    "json_list = []  # List to store the JSON objects\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        json_list.append(json_obj)\n",
    "# Create a dictionary with the desired format\n",
    "from datasets import Dataset\n",
    "\n",
    "data_dict = {\n",
    "    'document': [item['text'] for item in json_list],\n",
    "    'summary': [item['summary'] for item in json_list],\n",
    "    'id': [item['id'] for item in json_list],\n",
    "}\n",
    "\n",
    "# Create a Dataset object\n",
    "train = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Print the dataset information\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'arabic_val.jsonl'\n",
    "import json\n",
    "json_list = []  # List to store the JSON objects\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        json_list.append(json_obj)\n",
    "# Create a dictionary with the desired format\n",
    "from datasets import Dataset\n",
    "\n",
    "data_dict = {\n",
    "    'document': [item['text'] for item in json_list],\n",
    "    'summary': [item['summary'] for item in json_list],\n",
    "    'id': [item['id'] for item in json_list],\n",
    "}\n",
    "\n",
    "# Create a Dataset object\n",
    "validation = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Print the dataset information\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Create a DatasetDict object\n",
    "data_dict = DatasetDict({\n",
    "    'train': train,\n",
    "    'test': validation\n",
    "})\n",
    "\n",
    "# Print the dataset information\n",
    "print(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bf34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percentage of the dataset you want to split as train and test\n",
    "TRAIN_TEST_SPLIT = 0.1\n",
    "\n",
    "MAX_INPUT_LENGTH = 1024  # Maximum length of the input to the model\n",
    "MIN_TARGET_LENGTH = 5  # Minimum length of the output by the model\n",
    "MAX_TARGET_LENGTH = 128  # Maximum length of the output by the model\n",
    "BATCH_SIZE = 4  # Batch-size for training our model\n",
    "LEARNING_RATE = 5e-2  # Learning-rate for training our model\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs we will train the model for\n",
    "\n",
    "# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\n",
    "model_name = \"UBC-NLP/Arat5-msa-small\"\n",
    "MODEL_CHECKPOINT = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# raw_datasets = load_dataset(\"xsum\", split=\"train\")\n",
    "raw_datasets = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\" , \"UBC-NLP/AraT5-base\",\"UBC-NLP/Arat5-msa-small\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab11cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a86226",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d088911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "generation_dataset = (\n",
    "    tokenized_datasets[\"test\"]\n",
    "    .shuffle()\n",
    "    .select(list(range(200)))\n",
    "    .to_tf_dataset(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038681b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 0\n",
    "max_target_length = 0\n",
    "\n",
    "for example in tokenized_datasets[\"train\"]:\n",
    "    input_length = len(example[\"document\"])\n",
    "    target_length = len(example[\"summary\"])\n",
    "    if input_length > max_input_length:\n",
    "        max_input_length = input_length\n",
    "    if target_length > max_target_length:\n",
    "        max_target_length = target_length\n",
    "\n",
    "print(\"Max Input Length:\", max_input_length)\n",
    "print(\"Max Target Length:\", max_target_length)\n",
    "\n",
    "min_input_length = float(\"inf\")\n",
    "min_target_length = float(\"inf\")\n",
    "\n",
    "for example in tokenized_datasets[\"train\"]:\n",
    "    input_length = len(example[\"document\"])\n",
    "    target_length = len(example[\"summary\"])\n",
    "    if input_length < min_input_length:\n",
    "        min_input_length = input_length\n",
    "    if target_length < min_target_length:\n",
    "        min_target_length = target_length\n",
    "\n",
    "print(\"Min Input Length:\", min_input_length)\n",
    "print(\"Min Target Length:\", min_target_length)\n",
    "total_input_length = 0\n",
    "total_target_length = 0\n",
    "num_examples = 0\n",
    "\n",
    "for example in tokenized_datasets[\"train\"]:\n",
    "    input_length = len(example[\"document\"])\n",
    "    target_length = len(example[\"summary\"])\n",
    "    total_input_length += input_length\n",
    "    total_target_length += target_length\n",
    "    num_examples += 1\n",
    "\n",
    "mean_input_length = total_input_length / num_examples\n",
    "mean_target_length = total_target_length / num_examples\n",
    "\n",
    "print(\"Mean Input Length:\", mean_input_length)\n",
    "print(\"Mean Target Length:\", mean_target_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "\n",
    "rouge_l = keras_nlp.metrics.RougeL()\n",
    "\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge_l(decoded_labels, decoded_predictions)\n",
    "    # We will print only the F1 score, you can use other aggregation metrics as well\n",
    "    result = {\"RougeL\": result[\"f1_score\"]}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import math\n",
    "\n",
    "# Define your learning rate decay function\n",
    "def lr_decay(epoch):\n",
    "    initial_learning_rate = 0.0001  # Initial learning rate\n",
    "    decay_rate = 0.1  # Decay rate\n",
    "    decay_steps = 10  # Decay steps\n",
    "    new_learning_rate = initial_learning_rate * math.pow(decay_rate, math.floor(epoch / decay_steps))\n",
    "    return new_learning_rate\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",
    ")\n",
    "\n",
    "# Define the path where you want to save the weights\n",
    "checkpoint_path = 'model_weightslllll.h5'\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_decay)\n",
    "\n",
    "callbacks = [metric_callback, checkpoint_callback, lr_scheduler_callback]\n",
    "\n",
    "model.load_weights('decay_model_weights_052.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ceba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     train_dataset, validation_data=test_dataset, epochs=1, callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a305b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('decay_model_weights_052.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61d1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T23:00:25.130190Z",
     "start_time": "2023-07-09T23:00:19.847985Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "file_path = 'validation_data.jsonl'\n",
    "json_list = []  # List to store the JSON objects\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        json_list.append(json_obj)\n",
    "\n",
    "# Define the column name for the new summary\n",
    "new_summary_column = 'summary'\n",
    "\n",
    "# Create a new list to store the updated JSON data\n",
    "updated_json_list = []\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
    "\n",
    "# Iterate over each text in json_list\n",
    "for i in range(len(json_list)):\n",
    "    print(f\"start {i}\")\n",
    "    print(f\"paragraph:- {json_list[i]['paragraph']}\")\n",
    "    \n",
    "    # Generate the summary for the current text\n",
    "    summary = summarizer(\n",
    "        json_list[i]['paragraph'],\n",
    "        min_length=MIN_TARGET_LENGTH,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "    )[0]['summary_text']\n",
    "    \n",
    "    print(f\"new summary:- {summary}\")\n",
    "    # Create a new dictionary to store the updated data\n",
    "    updated_dict = dict(json_list[i])\n",
    "\n",
    "    # Add the summary to the new dictionary\n",
    "    updated_dict[new_summary_column] = summary\n",
    "\n",
    "    # Append the updated dictionary to the new list\n",
    "    updated_json_list.append(updated_dict)\n",
    "    print(f\"finished {i}\")\n",
    "\n",
    "# Save the updated JSON data to a file in JSONL format\n",
    "output_file_path = 'predictions.jsonl'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for updated_dict in updated_json_list:\n",
    "        updated_dict_str = json.dumps(updated_dict, ensure_ascii=False) + '\\n'\n",
    "        output_file.write(updated_dict_str)\n",
    "\n",
    "print(f\"Updated JSON data saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60834e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09a3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-09T23:11:42.192646Z",
     "start_time": "2023-07-09T23:09:19.688481Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "model_name=\"abdalrahmanshahrour/arabartsummarization\"\n",
    "preprocessor = ArabertPreprocessor(model_name=\"\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pipeline = pipeline(\"text2text-generation\",model=model,tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "file_path = 'validation_data.jsonl'\n",
    "\n",
    "json_list = []  # List to store the JSON objects\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        json_list.append(json_obj)\n",
    "        \n",
    "for i in range (len(json_list)):    \n",
    "    text = json_list[i]['paragraph']\n",
    "    text = preprocessor.preprocess(text)\n",
    "\n",
    "    result = pipeline(text,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_beams=3,\n",
    "                repetition_penalty=3.0,\n",
    "                max_length=200,\n",
    "                length_penalty=1.0,\n",
    "                no_repeat_ngram_size = 3)[0]['generated_text']\n",
    "    json_list[i]['summary'] = result\n",
    "#     del json_list[i][\"paragraph\"]\n",
    "    print(f'paragraph : {json_list[i][\"paragraph\"]}')\n",
    "    print(f'our_summary : {json_list[i][\"summary\"]}')\n",
    "\n",
    "gg = json_list\n",
    "with jsonlines.open('predictions.jsonl', mode='w') as writer:\n",
    "    writer.write_all(gg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "file_path = 'validation_data.jsonl'\n",
    "summary_list = []  # List to store the summaries\n",
    "nnn=[]\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        text = json_obj['paragraph']\n",
    "        candidate_summaries = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            sentences = re.split(r'(?<=[.:;])\\s', text)[:i + 1]\n",
    "            summary = ' '.join(sentences)\n",
    "            candidate_summaries.append(summary)\n",
    "            if i ==2:\n",
    "                nnn.append(candidate_summaries[2])\n",
    "        summary_list.append(candidate_summaries)\n",
    "        \n",
    "\n",
    "# summary_list\n",
    "file_path = 'validation_data.jsonl'\n",
    "import json\n",
    "json_list = []  # List to store the JSON objects\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        json_list.append(json_obj)\n",
    "# json_list[0]\n",
    "\n",
    "for i in range (len(json_list)):\n",
    "    json_list[i]['summary'] = nnn[i]\n",
    "    del json_list[i][\"paragraph\"]\n",
    "    \n",
    "# json_list[0]\n",
    "\n",
    "import jsonlines\n",
    "gg = json_list\n",
    "with jsonlines.open('predictions.jsonl', mode='w') as writer:\n",
    "    writer.write_all(gg)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
